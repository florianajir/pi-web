services:
  watchtower:
    image: containrrr/watchtower:1.7.1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_SCHEDULE=0 0 3 * * *
      - WATCHTOWER_NO_RESTART=true
      - TZ=${TIMEZONE}
    healthcheck:
      test: ["CMD", "/watchtower", "--health-check"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  traefik:
    image: traefik:v3.4
    container_name: pi-traefik
    hostname: pi-traefik
    restart: unless-stopped
    command:
      - "--log.level=INFO"
      - "--api.dashboard=true"
      - "--ping=true"
      # Entry Points
      - "--entrypoints.web.address=:80/tcp"
      - "--entrypoints.web.http.redirections.entrypoint.to=websecure"
      - "--entrypoints.web.http.redirections.entrypoint.scheme=https"
      - "--entrypoints.websecure.address=:443/tcp"
      - "--entrypoints.websecure.http.middlewares=compress@docker,hsts@docker"
      - "--entrypoints.websecure.http3=true"
      # Providers
      - "--providers.docker=true"
      - "--providers.docker.watch=true"
      - "--providers.docker.network=frontend"
      - "--providers.docker.exposedbydefault=false"
      # Servers Transport
      - "--serversTransport.insecureSkipVerify=true" # corrected flag name for v3
      # (Removed invalid top-level --tls.options.* flags for v3; define via dynamic file if needed)
    labels:
      - "com.example.description=Traefik reverse proxy"
      - "com.example.service=traefik"
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      # Middlewares definitions
      - "traefik.http.middlewares.compress.compress=true"
      - "traefik.http.middlewares.hsts.headers.stsSeconds=2592000"
      # - "traefik.http.middlewares.lan.ipallowlist.sourcerange=${ALLOW_IP_RANGES:-192.168.1.0/24}"
      # Traefik Dashboard Router
      # - "traefik.http.routers.traefik.middlewares=lan@docker"
      - "traefik.http.routers.traefik.entrypoints=websecure"
      - "traefik.http.routers.traefik.rule=Host(`traefik.${HOST_NAME:-pi.lan}`)"
      - "traefik.http.routers.traefik.service=api@internal"
      - "traefik.http.routers.traefik.tls=true"
      - "traefik.http.services.traefik.loadbalancer.server.port=8080"
    ports:
      - "80:80"
      - "443:443"
    expose:
      - 8080
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - frontend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "-q", "-O", "/dev/null", "http://127.0.0.1:8080/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    mem_limit: 256m

  netdata:
    image: netdata/netdata:v2.8
    container_name: pi-netdata
    hostname: pi-netdata
    restart: unless-stopped
    pid: host
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    networks:
      - frontend
    expose:
      - 19999
    volumes:
      - netdata_config:/etc/netdata
      - netdata_lib:/var/lib/netdata
      - netdata_cache:/var/cache/netdata
      - /:/host/root:ro,rslave
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /etc/localtime:/etc/localtime:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/log:/host/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - NETDATA_CLAIM_TOKEN=${NETDATA_CLAIM_TOKEN:-}
      - NETDATA_CLAIM_URL=${NETDATA_CLAIM_URL:-}
      - NETDATA_CLAIM_ROOMS=${NETDATA_CLAIM_ROOMS:-}
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:19999/api/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "com.example.description=Netdata real-time monitoring"
      - "com.example.service=monitoring"
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      - "traefik.http.routers.netdata.rule=Host(`netdata.${HOST_NAME:-pi.lan}`)"
      - "traefik.http.routers.netdata.entrypoints=websecure"
      - "traefik.http.routers.netdata.tls=true"
      - "traefik.http.services.netdata.loadbalancer.server.port=19999"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    mem_limit: 512m

  portainer:
    image: portainer/portainer-ce:latest
    container_name: pi-portainer
    hostname: pi-portainer
    restart: unless-stopped
    networks:
      - frontend
    expose:
      - 9000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    labels:
      - "com.example.description=Portainer container management"
      - "com.example.service=monitoring"
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      - "traefik.http.routers.portainer.rule=Host(`portainer.${HOST_NAME:-pi.lan}`)"
      - "traefik.http.routers.portainer.entrypoints=websecure"
      - "traefik.http.routers.portainer.tls=true"
      - "traefik.http.services.portainer.loadbalancer.server.port=9000"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "/portainer", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    mem_limit: 256m

  pihole:
    image: pihole/pihole:latest
    container_name: pi-pihole
    hostname: pi-pihole
    restart: unless-stopped
    cap_add:
      - NET_ADMIN
      - SYS_NICE
    expose:
      - 53
      - 8082
    # Pi-hole expects to start some components as root then drop privileges. Remove forced user.
    environment:
      TZ: ${TIMEZONE:-Europe/Paris}
      FTLCONF_webserver_api_password: ${PASSWORD:-admin}
      FTLCONF_dns_listeningMode: "all"
      FTLCONF_dns_upstreams: "1.1.1.1;8.8.8.8"
      # Wildcard all hosts under HOST_NAME to the host (Traefik) IP. Pi-hole still keeps its own macvlan IP (IP var).
      FTLCONF_misc_dnsmasq_lines: "address=/${HOST_NAME:-pi.lan}/${HOST_IP:-192.168.1.30}"
      FTLCONF_webserver_port: 8082
      # DHCP configuration
      FTLCONF_dhcp_active: ${DHCP_ACTIVE:-false}
      FTLCONF_dhcp_start: ${DHCP_START:-192.168.1.100}
      FTLCONF_dhcp_end: ${DHCP_END:-192.168.1.150}
      FTLCONF_dhcp_router: ${DHCP_ROUTER:-192.168.1.1}
      FTLCONF_dhcp_leasetime: ${DHCP_LEASE_TIME:-24}
      FTLCONF_dhcp_ipv6: "false"
      FTLCONF_dhcp_rapidCommit: "true"
      FTLCONF_dns_domain: ${HOST_NAME:-pi.lan}
    volumes:
      - pihole_data:/etc/pihole
      - ./config/pihole/dnsmasq.d:/etc/dnsmasq.d
    networks:
      frontend: {}
      lan:
        ipv4_address: ${PIHOLE_IP:-192.168.1.29}
    labels:
      - "com.example.description=Pi-hole DNS Ad-Blocker"
      - "com.example.service=pihole"
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      - "traefik.http.routers.pihole.rule=Host(`pihole.${HOST_NAME:-pi.lan}`)"
      - "traefik.http.routers.pihole.entrypoints=websecure"
      - "traefik.http.routers.pihole.tls=true"
      # - "traefik.http.routers.pihole.middlewares=lan"
      - "traefik.http.services.pihole.loadbalancer.server.port=8082"
    healthcheck:
      test: ["CMD", "dig", "@127.0.0.1", "cloudflare.com"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    mem_limit: 512m

  n8n:
    image: n8nio/n8n:stable
    container_name: pi-n8n
    hostname: pi-n8n
    restart: unless-stopped
    expose:
      - 5678
    labels:
      - "com.example.description=n8n workflow automation"
      - "com.example.service=n8n"
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      - "traefik.http.routers.n8n.rule=Host(`n8n.${HOST_NAME:-pi.lan}`)"
      - "traefik.http.routers.n8n.entrypoints=websecure"
      - "traefik.http.routers.n8n.tls=true"
      - "traefik.http.routers.n8n.middlewares=n8n@docker"
      - "traefik.http.middlewares.n8n.headers.SSLRedirect=true"
      - "traefik.http.middlewares.n8n.headers.STSSeconds=315360000"
      - "traefik.http.middlewares.n8n.headers.browserXSSFilter=true"
      - "traefik.http.middlewares.n8n.headers.contentTypeNosniff=true"
      - "traefik.http.middlewares.n8n.headers.forceSTSHeader=true"
      - "traefik.http.middlewares.n8n.headers.SSLHost=${HOST_NAME:-pi.lan}"
      - "traefik.http.middlewares.n8n.headers.STSIncludeSubdomains=true"
      - "traefik.http.middlewares.n8n.headers.STSPreload=true"
      - "traefik.http.services.n8n.loadbalancer.server.port=5678"
    environment:
      - N8N_HOST=n8n.${HOST_NAME:-pi.lan}
      - N8N_PORT=5678
      - N8N_PROTOCOL=https
      - NODE_ENV=production
      - WEBHOOK_URL=https://n8n.${HOST_NAME:-pi.lan}/
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE:-${TIMEZONE:-Europe/Paris}}
      - TZ=${TIMEZONE:-Europe/Paris}
      - N8N_RUNNERS_ENABLED=${N8N_RUNNERS_ENABLED:-true}
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=${N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS:-true}
      - DB_TYPE=${DB_TYPE:-sqlite}
      - DB_SQLITE_POOL_SIZE=${DB_SQLITE_POOL_SIZE:-2}
      - N8N_BLOCK_ENV_ACCESS_IN_NODE=${N8N_BLOCK_ENV_ACCESS_IN_NODE:-false}
    volumes:
      - n8n_data:/home/node/.n8n
      - ./data/n8n/files:/files
    networks:
      - frontend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      - traefik
    healthcheck:
      # n8n listens internally on plain HTTP. Use explicit IPv4 loopback to avoid IPv6 (::1) resolution issues.
      test:
        [
          "CMD",
          "wget",
          "-q",
          "-O",
          "/dev/null",
          "http://127.0.0.1:5678/healthz",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    mem_limit: 512m

  nextcloud:
    image: nextcloud:28-apache
    container_name: pi-nextcloud
    hostname: pi-nextcloud
    restart: unless-stopped
    environment:
      - NEXTCLOUD_ADMIN_USER=${USER:-admin}
      - NEXTCLOUD_ADMIN_PASSWORD=${PASSWORD:-admin}
      - NEXTCLOUD_ADMIN_EMAIL=${EMAIL:-admin@example.com}
      - NEXTCLOUD_TRUSTED_DOMAINS=nextcloud.${HOST_NAME:-pi.lan}
      - NEXTCLOUD_DATA_DIR=/var/www/html/data
      - OVERWRITEHOST=nextcloud.${HOST_NAME:-pi.lan}
      - OVERWRITEPROTOCOL=https
      - TRUSTED_PROXIES=${NEXTCLOUD_TRUSTED_PROXIES:-172.30.11.0/24}
      - MYSQL_HOST=nextcloud-db
      - MYSQL_DATABASE=${NEXTCLOUD_DB_NAME:-nextcloud}
      - MYSQL_USER=${NEXTCLOUD_DB_USER:-pi-web}
      - MYSQL_PASSWORD=${NEXTCLOUD_DB_PASSWORD:-pi-web}
      - REDIS_HOST=nextcloud-redis
    volumes:
      - nextcloud_data:/var/www/html
    networks:
      - frontend
      - nextcloud
    labels:
      - "com.example.description=Nextcloud self-hosted storage"
      - "com.example.service=nextcloud"
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      - "traefik.http.routers.nextcloud.rule=Host(`nextcloud.${HOST_NAME:-pi.lan}`)"
      - "traefik.http.routers.nextcloud.entrypoints=websecure"
      - "traefik.http.routers.nextcloud.tls=true"
      - "traefik.http.services.nextcloud.loadbalancer.server.port=80"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/status.php"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      - nextcloud-db
      - nextcloud-redis
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    mem_limit: 1g

  nextcloud-db:
    image: mariadb:11.4
    container_name: pi-nextcloud-db
    hostname: pi-nextcloud-db
    restart: unless-stopped
    environment:
      - MYSQL_ROOT_PASSWORD=${NEXTCLOUD_DB_ROOT_PASSWORD:-pi-web-root}
      - MYSQL_DATABASE=${NEXTCLOUD_DB_NAME:-nextcloud}
      # Use same default user as Nextcloud app service to avoid mismatch after wiping volume
      - MYSQL_USER=${NEXTCLOUD_DB_USER:-nextcloud}
      - MYSQL_PASSWORD=${NEXTCLOUD_DB_PASSWORD:-pi-web}
    volumes:
      - nextcloud_db:/var/lib/mysql
    networks:
      - nextcloud
    healthcheck:
      # SQL-level readiness check using root credentials (env-substituted at compose render time).
      # Uses a simple SELECT 1 which will fail until the server completed initialization.
      test:
        [
          "CMD",
          "mariadb",
          "-uroot",
          "-p${NEXTCLOUD_DB_ROOT_PASSWORD:-pi-web-root}",
          "-e",
          "SELECT 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    mem_limit: 512m

  nextcloud-redis:
    image: redis:7-alpine
    container_name: pi-nextcloud-redis
    hostname: pi-nextcloud-redis
    restart: unless-stopped
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    networks:
      - nextcloud
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    mem_limit: 128m

  # Connectivity Stack
  headscale:
    image: headscale/headscale:stable
    container_name: pi-headscale
    hostname: pi-headscale
    restart: unless-stopped
    command: serve
    environment:
      - HEADSCALE_SERVER_URL=https://headscale.${HOST_NAME:-pi.lan}
      - HEADSCALE_LISTEN_ADDR=0.0.0.0:8080
      - HEADSCALE_DNS_BASE_DOMAIN=vpn.${HOST_NAME:-pi.lan}
    networks:
      - frontend
    expose:
      - 8080
      - 9090
      - 50443
    ports:
      - "8080:8080"
      - "3478:3478/udp"
    volumes:
      - ./config/headscale:/etc/headscale:ro
      - headscale_data:/var/lib/headscale
      - headscale_run:/var/run/headscale
    labels:
      - "com.example.description=Headscale coordination server"
      - "com.example.service=vpn"
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      - "traefik.http.routers.headscale.rule=Host(`headscale.${HOST_NAME:-pi.lan}`)"
      - "traefik.http.routers.headscale.entrypoints=websecure"
      - "traefik.http.routers.headscale.tls=true"
      - "traefik.http.services.headscale.loadbalancer.server.port=8080"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "/ko-app/headscale", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    mem_limit: 256m

  tailscale:
    image: tailscale/tailscale:stable
    container_name: pi-tailscale
    hostname: pi-tailscale
    restart: unless-stopped
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY:-}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=${TS_USERSPACE:-false}
      - TS_EXTRA_ARGS=${TS_EXTRA_ARGS:---login-server=https://headscale:8080 --accept-dns=false --advertise-exit-node --accept-routes}
      - TS_HOSTNAME=tailscale.${HOST_NAME:-pi.lan}
    volumes:
      - tailscale_state:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    networks:
      - frontend
    depends_on:
      - headscale
    healthcheck:
      test: ["CMD", "tailscale", "status", "--peers=false"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    labels:
      - "com.example.description=Tailscale VPN client (Headscale)"
      - "com.example.service=vpn"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    mem_limit: 256m

volumes:
  netdata_config:
    labels:
      - "com.example.description=Netdata Configuration"
      - "com.example.service=monitoring"
  netdata_lib:
    labels:
      - "com.example.description=Netdata Library Data"
      - "com.example.service=monitoring"
  netdata_cache:
    labels:
      - "com.example.description=Netdata Cache Data"
      - "com.example.service=monitoring"
  portainer_data:
    labels:
      - "com.example.description=Portainer Persistent Data"
      - "com.example.service=monitoring"
  pihole_data:
    labels:
      - "com.example.description=Pi-hole Configuration Data"
      - "com.example.service=dns"
  n8n_data:
    labels:
      - "com.example.description=n8n Persistent Data"
      - "com.example.service=automation"
  nextcloud_data:
    labels:
      - "com.example.description=Nextcloud Application Data"
      - "com.example.service=cloud"
  nextcloud_db:
    labels:
      - "com.example.description=Nextcloud Database Data"
      - "com.example.service=cloud"
  headscale_data:
    labels:
      - "com.example.description=Headscale Database and Keys"
      - "com.example.service=vpn"
  headscale_run:
    labels:
      - "com.example.description=Headscale Runtime Socket"
      - "com.example.service=vpn"
  tailscale_state:
    labels:
      - "com.example.description=Tailscale Persistent State"
      - "com.example.service=vpn"

networks:
  frontend:
    driver: bridge
    # Force stable docker network name to match traefik provider config
    name: frontend
    ipam:
      driver: default
      config:
        - subnet: 172.30.11.0/24
          gateway: 172.30.11.1
    labels:
      - "com.example.description=External Web Network"
      - "com.example.service=proxy"
  lan:
    driver: macvlan
    driver_opts:
      parent: ${LAN_PARENT:-eth0}
    ipam:
      driver: default
      config:
        - subnet: ${LAN_SUBNET:-192.168.1.0/24}
          gateway: ${LAN_GATEWAY:-192.168.1.1}
    labels:
      - "com.example.description=Physical LAN macvlan for DHCP"
      - "com.example.service=network"
  nextcloud:
    driver: bridge
    internal: true
    labels:
      - "com.example.description=Nextcloud internal network"
      - "com.example.service=cloud"
